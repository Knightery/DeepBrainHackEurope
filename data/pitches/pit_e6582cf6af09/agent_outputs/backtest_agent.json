{
  "status": "success",
  "metrics": {
    "backtest_start": "2016-01-05",
    "backtest_end": "2018-12-07",
    "cagr": 0.016427,
    "total_return": 0.04875,
    "volatility": 0.074129,
    "sharpe_ratio": 0.256783,
    "sortino_ratio": 0.112508,
    "calmar_ratio": 0.173555,
    "max_drawdown": -0.094652,
    "max_drawdown_duration": 304,
    "total_trades": 36,
    "win_rate": 0.805556,
    "avg_win": 0.00751,
    "avg_loss": -0.022594,
    "profit_factor": 1.377076,
    "expectancy": 0.001657,
    "benchmark_cagr": 0.113267,
    "benchmark_max_drawdown": -0.100445,
    "benchmark_total_return": 0.36814,
    "alpha": 0.023229,
    "information_ratio": -0.647795,
    "excess_return": -0.09684,
    "up_capture": -0.001187,
    "down_capture": -0.033215,
    "name": "pit_e6582cf6af09",
    "ticker": "SPY"
  },
  "message": "Backtest completed successfully on attempt 1.",
  "attempt_count": 1,
  "attempt_history": [
    {
      "attempt": 1,
      "phase_reached": "review",
      "script": "import subprocess, sys\n\n# Install required packages\nfor pkg in [\"alpaca-trade-api\", \"ffn\"]:\n    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pkg],\n                          stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n\nimport os\nimport json\nimport numpy as np\nimport pandas as pd\nimport math\nimport sys\n\n# \u2500\u2500 verbatim Alpaca fetch \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\ndef _fetch_benchmark_alpaca(\n    ticker: str,\n    start: str,\n    end: str,\n    timeframe: str = \"1Day\",\n    adjustment: str = \"all\",\n    feed: str = \"sip\",\n    limit: int = 10000,\n    sort: str = \"asc\",\n) -> \"pd.DataFrame\":\n    import os, json, urllib.request, urllib.parse, urllib.error, re\n    api_key = os.getenv(\"ALPACA_API_KEY\", \"\")\n    api_secret = os.getenv(\"ALPACA_API_SECRET\", \"\")\n    if not api_key or not api_secret:\n        raise RuntimeError(\"ALPACA_API_KEY or ALPACA_API_SECRET environment variable is not set.\")\n\n    timeframe = str(timeframe or \"1Day\").strip()\n    if not re.match(r\"^[1-9][0-9]*(Min|Hour|Day|Week|Month)$\", timeframe):\n        raise RuntimeError(\n            f\"Invalid Alpaca timeframe '{timeframe}'. \"\n            \"Expected values like 1Min, 5Min, 15Min, 1Hour, 1Day.\"\n        )\n    adjustment = str(adjustment or \"all\").strip().lower()\n    if adjustment not in {\"raw\", \"split\", \"dividend\", \"all\"}:\n        raise RuntimeError(f\"Invalid Alpaca adjustment '{adjustment}'.\")\n    feed = str(feed or \"sip\").strip().lower()\n    if feed not in {\"sip\", \"iex\", \"otc\"}:\n        raise RuntimeError(f\"Invalid Alpaca feed '{feed}'.\")\n    sort = str(sort or \"asc\").strip().lower()\n    if sort not in {\"asc\", \"desc\"}:\n        raise RuntimeError(f\"Invalid Alpaca sort '{sort}'.\")\n    try:\n        limit = max(1, min(10000, int(limit)))\n    except Exception as exc:\n        raise RuntimeError(f\"Invalid Alpaca limit '{limit}'.\") from exc\n\n    base_url = \"https://data.alpaca.markets/v2/stocks/bars\"\n    headers = {\n        \"APCA-API-KEY-ID\": api_key,\n        \"APCA-API-SECRET-KEY\": api_secret,\n        \"Accept\": \"application/json\",\n    }\n    bars = []\n    page_token = None\n    while True:\n        params = {\n            \"symbols\": ticker,\n            \"timeframe\": timeframe,\n            \"start\": start,\n            \"end\": end,\n            \"adjustment\": adjustment,\n            \"feed\": feed,\n            \"limit\": str(limit),\n            \"sort\": sort,\n        }\n        if page_token:\n            params[\"page_token\"] = page_token\n        url = base_url + \"?\" + urllib.parse.urlencode(params)\n        req = urllib.request.Request(url, headers=headers)\n        try:\n            with urllib.request.urlopen(req, timeout=30) as resp:\n                data = json.loads(resp.read().decode())\n        except urllib.error.HTTPError as exc:\n            raise RuntimeError(f\"Alpaca API HTTP error {exc.code}: {exc.reason}\") from exc\n        except urllib.error.URLError as exc:\n            raise RuntimeError(f\"Alpaca API network error: {exc.reason}\") from exc\n\n        symbol_bars = data.get(\"bars\", {}).get(ticker.upper(), [])\n        bars.extend(symbol_bars)\n        page_token = data.get(\"next_page_token\")\n        if not page_token:\n            break\n\n    if not bars:\n        raise RuntimeError(\n            f\"Alpaca returned 0 bars for {ticker} between {start} and {end}. \"\n            \"Check that the ticker is valid and the date range covers trading days.\"\n        )\n\n    import pandas as pd\n    df = pd.DataFrame(bars)\n    df[\"date\"] = pd.to_datetime(df.get(\"t\"), errors=\"coerce\", utc=True).dt.tz_localize(None)\n    df[\"open\"] = pd.to_numeric(df.get(\"o\"), errors=\"coerce\")\n    df[\"high\"] = pd.to_numeric(df.get(\"h\"), errors=\"coerce\")\n    df[\"low\"] = pd.to_numeric(df.get(\"l\"), errors=\"coerce\")\n    df[\"close\"] = pd.to_numeric(df.get(\"c\"), errors=\"coerce\")\n    df[\"volume\"] = pd.to_numeric(df.get(\"v\"), errors=\"coerce\")\n    df[\"vwap\"] = pd.to_numeric(df.get(\"vw\"), errors=\"coerce\")\n    df[\"trade_count\"] = pd.to_numeric(df.get(\"n\"), errors=\"coerce\")\n    df = (\n        df[[\"date\", \"open\", \"high\", \"low\", \"close\", \"volume\", \"vwap\", \"trade_count\"]]\n        .dropna(subset=[\"date\", \"close\"])\n        .drop_duplicates(subset=[\"date\"])\n        .sort_values(\"date\")\n        .reset_index(drop=True)\n    )\n    if df.empty:\n        raise RuntimeError(f\"Alpaca bars for {ticker} contained no parseable close prices.\")\n\n    df[\"Open\"] = df[\"open\"]\n    df[\"High\"] = df[\"high\"]\n    df[\"Low\"] = df[\"low\"]\n    df[\"Close\"] = df[\"close\"]\n    df[\"Volume\"] = df[\"volume\"]\n    df[\"Adj Close\"] = df[\"close\"]\n    return df\n# \u2500\u2500 end verbatim \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\n\n# \u2500\u2500 Parameters \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nTICKER = \"SPY\"\nBACKTEST_START = \"1993-01-29\"   # SPY inception; strategy uses full history\nBACKTEST_END   = \"2018-12-08\"   # end date referenced in the notebook\nTIMEFRAME      = \"1Day\"\nANNUALIZATION  = 252\n\nprint(\"Fetching SPY daily data from Alpaca \u2026\", file=sys.stderr)\n\n# Fetch all available bars in multiple pages (limit=10000 per page)\nall_bars = []\nstart_cur = BACKTEST_START\nwhile True:\n    chunk = _fetch_benchmark_alpaca(\n        ticker=TICKER,\n        start=start_cur,\n        end=BACKTEST_END,\n        timeframe=TIMEFRAME,\n        adjustment=\"all\",\n        feed=\"sip\",\n        limit=10000,\n        sort=\"asc\",\n    )\n    all_bars.append(chunk)\n    # If we got fewer than 10000 rows, we're done\n    if len(chunk) < 10000:\n        break\n    # Advance start to day after last fetched date\n    last_date = chunk[\"date\"].iloc[-1]\n    next_start = (last_date + pd.Timedelta(days=1)).strftime(\"%Y-%m-%d\")\n    if next_start >= BACKTEST_END:\n        break\n    start_cur = next_start\n\nraw = pd.concat(all_bars, ignore_index=True).drop_duplicates(subset=[\"date\"]).sort_values(\"date\").reset_index(drop=True)\nprint(f\"Fetched {len(raw)} daily bars for {TICKER} ({raw['date'].iloc[0].date()} \u2013 {raw['date'].iloc[-1].date()})\", file=sys.stderr)\n\n# Build pricing DataFrame matching the notebook structure\npricing = pd.DataFrame()\npricing[\"Date\"]      = raw[\"date\"]\npricing[\"Adj_Open\"]  = raw[\"open\"]\npricing[\"Adj_Close\"] = raw[\"close\"]\npricing = pricing.reset_index(drop=True)\n\n# \u2500\u2500 Replicate notebook logic \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nstock = pricing.copy()\nstock[\"Adj_Close\"] = stock[\"Adj_Close\"].shift(1)\n\n# The notebook splits into stock_1/2/3 but only runs trade() on the full stock.\n# Following the actual executed cell: trade(stock, moving_average=10)\nmoving_average = 10\n\ndef trade(stock, length):\n    temp_dict = {}\n    if length == 0:\n        return 0\n\n    rolling_window = stock.Adj_Close.rolling(window=length)\n    mu  = rolling_window.mean()\n    sma = stock.Adj_Close.rolling(window=length * 1).mean()\n    lma = stock.Adj_Close.rolling(window=length * 10).mean()\n    std = rolling_window.std()\n\n    max_position      = 1\n    percent_per_trade = 1.0\n    slippage_adj      = 1\n\n    zscores = (stock.Adj_Close - mu) / std\n\n    money          = 1000.00\n    position_count = 0\n\n    for i, row in enumerate(stock.itertuples(), 0):\n        equity = money + (stock.Adj_Close[i] * position_count)\n        if equity > 0:\n            fixed_frac = (equity * percent_per_trade) / stock.Adj_Close[i]\n        else:\n            fixed_frac = 0\n        fixed_frac = int(round(fixed_frac))\n\n        if i > 0:\n            if (zscores[i - 1] > 0.5 and zscores[i] < -0.5) or (zscores[i - 1] < -0.5 and zscores[i] > 0.5):\n                if position_count > 0:\n                    money += position_count * stock.Adj_Close[i] * slippage_adj\n                elif position_count < 0:\n                    money += position_count * stock.Adj_Close[i] * (1 / slippage_adj)\n                position_count = 0\n\n        if (zscores[i] > 1) & (position_count > max_position * -1) & (sma[i] < lma[i]):\n            position_count -= fixed_frac\n            money += fixed_frac * stock.Adj_Close[i] * slippage_adj\n\n        elif zscores[i] < -1 and position_count < max_position and sma[i] > lma[i]:\n            position_count += fixed_frac\n            money -= fixed_frac * stock.Adj_Close[i] * (1 / slippage_adj)\n\n        elif abs(zscores[i]) < 0.5:\n            if position_count > 0:\n                money += position_count * stock.Adj_Close[i] * slippage_adj\n            elif position_count < 0:\n                money += position_count * stock.Adj_Close[i] * (1 / slippage_adj)\n            position_count = 0\n\n        temp_dict[stock.Date[i]] = [\n            stock.Adj_Open[i], stock.Adj_Close[i], mu[i], std[i], zscores[i],\n            money, position_count, fixed_frac, sma[i], lma[i]\n        ]\n\n    pr = pd.DataFrame(data=temp_dict).T\n    pr.index.name = \"Date\"\n    pr.index = pd.to_datetime(pr.index)\n    pr.columns = [\"Open\", \"Close\", \"mu\", \"std\", \"zscores\", \"money\",\n                  \"position_count\", \"fixed_frac\", \"sma\", \"lma\"]\n    for col in pr.columns:\n        pr[col] = pd.to_numeric(pr[col], errors=\"coerce\")\n    pr[\"equity\"] = pr[\"money\"] + (pr[\"Close\"] * pr[\"position_count\"])\n    return pr\n\n\nprint(\"Running mean-reversion strategy \u2026\", file=sys.stderr)\nprofit = trade(stock, moving_average)\n\nprint(f\"Strategy produced {len(profit)} rows of results\", file=sys.stderr)\n\n# \u2500\u2500 Equity curve \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nequity_curve = profit[\"equity\"].copy().dropna()\nequity_curve = equity_curve[equity_curve > 0]\n\nbacktest_start_dt = equity_curve.index[0]\nbacktest_end_dt   = equity_curve.index[-1]\nbacktest_start    = backtest_start_dt.strftime(\"%Y-%m-%d\")\nbacktest_end      = backtest_end_dt.strftime(\"%Y-%m-%d\")\n\nprint(f\"Equity curve: {backtest_start} \u2013 {backtest_end}, \"\n      f\"start={equity_curve.iloc[0]:.2f}, end={equity_curve.iloc[-1]:.2f}\", file=sys.stderr)\n\n# \u2500\u2500 Daily returns \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\ndaily_returns = equity_curve.pct_change().dropna()\n\n# \u2500\u2500 Total & CAGR \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\ntotal_return = (equity_curve.iloc[-1] / equity_curve.iloc[0]) - 1.0\nn_days       = (backtest_end_dt - backtest_start_dt).days\nn_years      = n_days / 365.25\ncagr         = (1 + total_return) ** (1 / n_years) - 1 if n_years > 0 else 0.0\n\n# \u2500\u2500 Volatility & Sharpe \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nvolatility   = daily_returns.std() * math.sqrt(ANNUALIZATION)\nsharpe_ratio = (daily_returns.mean() / daily_returns.std() * math.sqrt(ANNUALIZATION)\n                if daily_returns.std() > 0 else 0.0)\n\n# \u2500\u2500 Sortino \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\ndownside_returns = daily_returns[daily_returns < 0]\ndownside_std     = downside_returns.std() * math.sqrt(ANNUALIZATION) if len(downside_returns) > 1 else np.nan\nsortino_ratio    = (cagr / downside_std) if (downside_std and not np.isnan(downside_std)) else 0.0\n\n# \u2500\u2500 Max drawdown \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nrolling_max  = equity_curve.cummax()\ndrawdown_ser = (equity_curve - rolling_max) / rolling_max\nmax_drawdown = drawdown_ser.min()   # negative\n\n# Max drawdown duration (calendar days)\nin_dd          = drawdown_ser < 0\ndd_dur_days    = 0\ncur_dur        = 0\ndd_start       = None\nfor dt, val in drawdown_ser.items():\n    if val < 0:\n        if dd_start is None:\n            dd_start = dt\n        cur_dur = (dt - dd_start).days\n        dd_dur_days = max(dd_dur_days, cur_dur)\n    else:\n        dd_start = None\n        cur_dur  = 0\n\ncalmar_ratio = (cagr / abs(max_drawdown)) if max_drawdown != 0 else 0.0\n\n# \u2500\u2500 Trade extraction \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nposition_series = profit[\"position_count\"].copy()\nclose_series    = profit[\"Close\"].copy()\n\ntrades   = []\npos_prev = 0.0\nentry_px = None\nentry_pos = 0.0\n\nfor dt, pos in position_series.items():\n    pos = float(pos)\n    px  = float(close_series[dt])\n    chg = pos - pos_prev\n\n    if pos_prev == 0 and pos != 0:\n        # New position opened\n        entry_px  = px\n        entry_pos = pos\n    elif pos_prev != 0 and pos == 0:\n        # Position closed\n        if entry_px and entry_px != 0:\n            if entry_pos > 0:\n                ret = (px - entry_px) / entry_px\n            else:\n                ret = (entry_px - px) / entry_px\n            trades.append(ret)\n        entry_px  = None\n        entry_pos = 0.0\n    elif pos_prev != 0 and pos != 0 and np.sign(pos) != np.sign(pos_prev):\n        # Flip\n        if entry_px and entry_px != 0:\n            if entry_pos > 0:\n                ret = (px - entry_px) / entry_px\n            else:\n                ret = (entry_px - px) / entry_px\n            trades.append(ret)\n        entry_px  = px\n        entry_pos = pos\n\n    pos_prev = pos\n\n# Close any remaining open position\nif pos_prev != 0 and entry_px and entry_px != 0:\n    px = float(close_series.iloc[-1])\n    if entry_pos > 0:\n        ret = (px - entry_px) / entry_px\n    else:\n        ret = (entry_px - px) / entry_px\n    trades.append(ret)\n\ntotal_trades = len(trades)\nprint(f\"Total trades extracted: {total_trades}\", file=sys.stderr)\n\nif total_trades > 0:\n    wins       = [t for t in trades if t > 0]\n    losses     = [t for t in trades if t <= 0]\n    win_rate   = len(wins) / total_trades\n    avg_win    = float(np.mean(wins))   if wins   else 0.0\n    avg_loss   = float(np.mean(losses)) if losses else 0.0\n    sum_wins   = sum(wins)\n    sum_losses = abs(sum(losses))\n    profit_factor = (sum_wins / sum_losses) if sum_losses > 0 else float(\"inf\")\n    expectancy    = win_rate * avg_win + (1 - win_rate) * avg_loss\nelse:\n    win_rate = avg_win = avg_loss = profit_factor = expectancy = 0.0\n\n# \u2500\u2500 Benchmark \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nprint(\"Fetching benchmark (SPY buy-and-hold) \u2026\", file=sys.stderr)\n\nbench_bars = []\nb_start_cur = backtest_start\nwhile True:\n    chunk = _fetch_benchmark_alpaca(\n        ticker=TICKER,\n        start=b_start_cur,\n        end=backtest_end,\n        timeframe=\"1Day\",\n        adjustment=\"all\",\n        feed=\"sip\",\n        limit=10000,\n        sort=\"asc\",\n    )\n    bench_bars.append(chunk)\n    if len(chunk) < 10000:\n        break\n    last_date = chunk[\"date\"].iloc[-1]\n    next_start = (last_date + pd.Timedelta(days=1)).strftime(\"%Y-%m-%d\")\n    if next_start >= backtest_end:\n        break\n    b_start_cur = next_start\n\nbench_df = pd.concat(bench_bars, ignore_index=True).drop_duplicates(subset=[\"date\"]).sort_values(\"date\").reset_index(drop=True)\n\nbench_close = bench_df.set_index(\"date\")[\"close\"].dropna()\nbench_total_return = (bench_close.iloc[-1] / bench_close.iloc[0]) - 1.0\nbench_n_days  = (bench_close.index[-1] - bench_close.index[0]).days\nbench_n_years = bench_n_days / 365.25\nbenchmark_cagr = (1 + bench_total_return) ** (1 / bench_n_years) - 1 if bench_n_years > 0 else 0.0\n\nbench_rolling_max = bench_close.cummax()\nbench_dd_ser      = (bench_close - bench_rolling_max) / bench_rolling_max\nbenchmark_max_drawdown = float(bench_dd_ser.min())\n\n# \u2500\u2500 Alpha & Information Ratio \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nbench_daily = bench_close.pct_change().dropna()\n\n# Align strategy and benchmark on common dates\nstrat_daily_aligned, bench_daily_aligned = daily_returns.align(bench_daily, join=\"inner\")\n\nif len(strat_daily_aligned) > 2:\n    excess_daily = strat_daily_aligned - bench_daily_aligned\n    # Jensen's alpha: regression of strategy excess on bench excess\n    from numpy.polynomial import polynomial as P\n    bench_excess = bench_daily_aligned - 0.0   # rf=0\n    cov_mat  = np.cov(strat_daily_aligned.values, bench_daily_aligned.values)\n    beta     = cov_mat[0, 1] / cov_mat[1, 1] if cov_mat[1, 1] != 0 else 0.0\n    alpha_daily = strat_daily_aligned.mean() - beta * bench_daily_aligned.mean()\n    alpha        = float(alpha_daily * ANNUALIZATION)\n\n    ir_mean  = excess_daily.mean()\n    ir_std   = excess_daily.std()\n    information_ratio = float((ir_mean / ir_std) * math.sqrt(ANNUALIZATION)) if ir_std > 0 else 0.0\n    excess_return = cagr - benchmark_cagr\n\n    # Up/Down capture\n    bench_up   = bench_daily_aligned[bench_daily_aligned > 0]\n    bench_down = bench_daily_aligned[bench_daily_aligned < 0]\n    strat_on_up   = strat_daily_aligned[bench_daily_aligned > 0]\n    strat_on_down = strat_daily_aligned[bench_daily_aligned < 0]\n\n    up_capture   = (strat_on_up.mean()   / bench_up.mean())   if len(bench_up)   > 0 and bench_up.mean()   != 0 else 0.0\n    down_capture = (strat_on_down.mean() / bench_down.mean()) if len(bench_down) > 0 and bench_down.mean() != 0 else 0.0\nelse:\n    alpha = information_ratio = excess_return = up_capture = down_capture = 0.0\n\n# \u2500\u2500 Assemble output \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\noutput = {\n    \"backtest_start\":        backtest_start,\n    \"backtest_end\":          backtest_end,\n    \"cagr\":                  round(float(cagr),            6),\n    \"total_return\":          round(float(total_return),    6),\n    \"volatility\":            round(float(volatility),      6),\n    \"sharpe_ratio\":          round(float(sharpe_ratio),    6),\n    \"sortino_ratio\":         round(float(sortino_ratio),   6),\n    \"calmar_ratio\":          round(float(calmar_ratio),    6),\n    \"max_drawdown\":          round(float(max_drawdown),    6),\n    \"max_drawdown_duration\": int(dd_dur_days),\n    \"total_trades\":          int(total_trades),\n    \"win_rate\":              round(float(win_rate),        6),\n    \"avg_win\":               round(float(avg_win),         6),\n    \"avg_loss\":              round(float(avg_loss),        6),\n    \"profit_factor\":         round(float(profit_factor),   6),\n    \"expectancy\":            round(float(expectancy),      6),\n    \"benchmark_cagr\":        round(float(benchmark_cagr),  6),\n    \"benchmark_max_drawdown\":round(float(benchmark_max_drawdown), 6),\n    \"benchmark_total_return\":round(float(bench_total_return),     6),\n    \"alpha\":                 round(float(alpha),            6),\n    \"information_ratio\":     round(float(information_ratio),6),\n    \"excess_return\":         round(float(excess_return),   6),\n    \"up_capture\":            round(float(up_capture),      6),\n    \"down_capture\":          round(float(down_capture),    6),\n}\n\nprint(json.dumps(output))",
      "stdout": "{\"backtest_start\": \"2016-01-05\", \"backtest_end\": \"2018-12-07\", \"cagr\": 0.016427, \"total_return\": 0.04875, \"volatility\": 0.074129, \"sharpe_ratio\": 0.256783, \"sortino_ratio\": 0.112508, \"calmar_ratio\": 0.173555, \"max_drawdown\": -0.094652, \"max_drawdown_duration\": 304, \"total_trades\": 36, \"win_rate\": 0.805556, \"avg_win\": 0.00751, \"avg_loss\": -0.022594, \"profit_factor\": 1.377076, \"expectancy\": 0.001657, \"benchmark_cagr\": 0.113267, \"benchmark_max_drawdown\": -0.100445, \"benchmark_total_return\": 0.36814, \"alpha\": 0.023229, \"information_ratio\": -0.647795, \"excess_return\": -0.09684, \"up_capture\": -0.001187, \"down_capture\": -0.033215}\n",
      "stderr": "Fetching SPY daily data from Alpaca \u2026\nFetched 739 daily bars for SPY (2016-01-04 \u2013 2018-12-07)\nRunning mean-reversion strategy \u2026\nStrategy produced 739 rows of results\nEquity curve: 2016-01-05 \u2013 2018-12-07, start=1000.00, end=1048.75\nTotal trades extracted: 36\nFetching benchmark (SPY buy-and-hold) \u2026\n",
      "returncode": 0,
      "review_verdict": "success",
      "review_feedback": "",
      "review_message": "",
      "latency_ms": 159311
    }
  ]
}