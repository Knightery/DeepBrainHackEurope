{
  "status": "success",
  "metrics": {
    "backtest_start": "2021-06-16",
    "backtest_end": "2026-02-20",
    "cagr": 0.036215,
    "total_return": 0.180592,
    "volatility": 0.085352,
    "sharpe_ratio": -0.009076,
    "sortino_ratio": -0.007878,
    "calmar_ratio": 0.230654,
    "max_drawdown": -0.157011,
    "max_drawdown_duration": 931,
    "total_trades": 8,
    "win_rate": 0.375,
    "avg_win": 0.078012,
    "avg_loss": -0.013296,
    "profit_factor": 3.520291,
    "expectancy": 0.020944,
    "benchmark_cagr": 0.125261,
    "benchmark_max_drawdown": -0.24512,
    "benchmark_total_return": 0.734528,
    "alpha": 0.005959,
    "information_ratio": -0.633309,
    "excess_return": -0.089045,
    "up_capture": 0.347858,
    "down_capture": 0.355546,
    "name": "pit_27dd416b88a7",
    "ticker": "SPY"
  },
  "message": "Backtest completed successfully on attempt 1.",
  "attempt_count": 1,
  "attempt_history": [
    {
      "attempt": 1,
      "phase_reached": "review",
      "script": "import subprocess, sys\nsubprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"numpy\", \"pandas\"], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n\nimport os\nimport io\nimport json\nimport numpy as np\nimport pandas as pd\nfrom datetime import datetime\n\n# \u2500\u2500 Inline Alpaca fetch function \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\ndef _fetch_benchmark_alpaca(\n    ticker: str,\n    start: str,\n    end: str,\n    timeframe: str = \"1Day\",\n    adjustment: str = \"all\",\n    feed: str = \"sip\",\n    limit: int = 10000,\n    sort: str = \"asc\",\n) -> \"pd.DataFrame\":\n    import os, json, urllib.request, urllib.parse, urllib.error, re\n    api_key = os.getenv(\"ALPACA_API_KEY\", \"\")\n    api_secret = os.getenv(\"ALPACA_API_SECRET\", \"\")\n    if not api_key or not api_secret:\n        raise RuntimeError(\"ALPACA_API_KEY or ALPACA_API_SECRET environment variable is not set.\")\n\n    timeframe = str(timeframe or \"1Day\").strip()\n    if not re.match(r\"^[1-9][0-9]*(Min|Hour|Day|Week|Month)$\", timeframe):\n        raise RuntimeError(\n            f\"Invalid Alpaca timeframe '{timeframe}'. \"\n            \"Expected values like 1Min, 5Min, 15Min, 1Hour, 1Day.\"\n        )\n    adjustment = str(adjustment or \"all\").strip().lower()\n    if adjustment not in {\"raw\", \"split\", \"dividend\", \"all\"}:\n        raise RuntimeError(f\"Invalid Alpaca adjustment '{adjustment}'.\")\n    feed = str(feed or \"sip\").strip().lower()\n    if feed not in {\"sip\", \"iex\", \"otc\"}:\n        raise RuntimeError(f\"Invalid Alpaca feed '{feed}'.\")\n    sort = str(sort or \"asc\").strip().lower()\n    if sort not in {\"asc\", \"desc\"}:\n        raise RuntimeError(f\"Invalid Alpaca sort '{sort}'.\")\n    try:\n        limit = max(1, min(10000, int(limit)))\n    except Exception as exc:\n        raise RuntimeError(f\"Invalid Alpaca limit '{limit}'.\") from exc\n\n    base_url = \"https://data.alpaca.markets/v2/stocks/bars\"\n    headers = {\n        \"APCA-API-KEY-ID\": api_key,\n        \"APCA-API-SECRET-KEY\": api_secret,\n        \"Accept\": \"application/json\",\n    }\n    bars = []\n    feed_candidates = [feed] if feed != \"sip\" else [\"sip\", \"iex\"]\n    last_http_error = None\n    chosen_feed = None\n    for feed_candidate in feed_candidates:\n        bars = []\n        page_token = None\n        request_failed = False\n        while True:\n            params = {\n                \"symbols\": ticker,\n                \"timeframe\": timeframe,\n                \"start\": start,\n                \"end\": end,\n                \"adjustment\": adjustment,\n                \"feed\": feed_candidate,\n                \"limit\": str(limit),\n                \"sort\": sort,\n            }\n            if page_token:\n                params[\"page_token\"] = page_token\n            url = base_url + \"?\" + urllib.parse.urlencode(params)\n            req = urllib.request.Request(url, headers=headers)\n            try:\n                with urllib.request.urlopen(req, timeout=30) as resp:\n                    data = json.loads(resp.read().decode())\n            except urllib.error.HTTPError as exc:\n                request_failed = True\n                last_http_error = (exc.code, exc.reason, feed_candidate)\n                break\n            except urllib.error.URLError as exc:\n                raise RuntimeError(f\"Alpaca API network error: {exc.reason}\") from exc\n\n            symbol_bars = data.get(\"bars\", {}).get(ticker.upper(), [])\n            bars.extend(symbol_bars)\n            page_token = data.get(\"next_page_token\")\n            if not page_token:\n                break\n\n        if not request_failed and bars:\n            chosen_feed = feed_candidate\n            break\n\n    if not bars:\n        if last_http_error is not None:\n            code, reason, failing_feed = last_http_error\n            raise RuntimeError(\n                f\"Alpaca API HTTP error {code}: {reason} (feed={failing_feed}, tried={feed_candidates})\"\n            )\n        raise RuntimeError(\n            f\"Alpaca returned 0 bars for {ticker} between {start} and {end}. \"\n            \"Check that the ticker is valid and the date range covers trading days.\"\n        )\n\n    df = pd.DataFrame(bars)\n    df[\"date\"] = pd.to_datetime(df.get(\"t\"), errors=\"coerce\", utc=True).dt.tz_localize(None)\n    df[\"open\"] = pd.to_numeric(df.get(\"o\"), errors=\"coerce\")\n    df[\"high\"] = pd.to_numeric(df.get(\"h\"), errors=\"coerce\")\n    df[\"low\"] = pd.to_numeric(df.get(\"l\"), errors=\"coerce\")\n    df[\"close\"] = pd.to_numeric(df.get(\"c\"), errors=\"coerce\")\n    df[\"volume\"] = pd.to_numeric(df.get(\"v\"), errors=\"coerce\")\n    df[\"vwap\"] = pd.to_numeric(df.get(\"vw\"), errors=\"coerce\")\n    df[\"trade_count\"] = pd.to_numeric(df.get(\"n\"), errors=\"coerce\")\n    df = (\n        df[[\"date\", \"open\", \"high\", \"low\", \"close\", \"volume\", \"vwap\", \"trade_count\"]]\n        .dropna(subset=[\"date\", \"close\"])\n        .drop_duplicates(subset=[\"date\"])\n        .sort_values(\"date\")\n        .reset_index(drop=True)\n    )\n    if df.empty:\n        raise RuntimeError(f\"Alpaca bars for {ticker} contained no parseable close prices.\")\n\n    df[\"Open\"] = df[\"open\"]\n    df[\"High\"] = df[\"high\"]\n    df[\"Low\"] = df[\"low\"]\n    df[\"Close\"] = df[\"close\"]\n    df[\"Volume\"] = df[\"volume\"]\n    df[\"Adj Close\"] = df[\"close\"]\n    df[\"alpaca_feed\"] = chosen_feed or feed\n    return df\n\n\n# \u2500\u2500 Parameters \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nTICKER = \"SPY\"\nSTART_DATE = \"2005-01-01\"\nEND_DATE = datetime.today().strftime(\"%Y-%m-%d\")\nTREND_WINDOW = 200\nYIELD_SPREAD_THRESHOLD = -0.10\nYIELD_SPREAD_SMOOTHING = 20\nTRANSACTION_COST = 0.0005\n\n# \u2500\u2500 Step 1: Fetch price data via Alpaca \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nprint(f\"Fetching price data for {TICKER} from Alpaca ({START_DATE} \u2192 {END_DATE})...\", file=sys.stderr)\nprice_df = _fetch_benchmark_alpaca(\n    ticker=TICKER,\n    start=START_DATE,\n    end=END_DATE,\n    timeframe=\"1Day\",\n    adjustment=\"all\",\n    feed=\"sip\",\n    limit=10000,\n    sort=\"asc\",\n)\nprint(f\"ALPACA_FETCH_OK timeframe=1Day rows={len(price_df)}\", file=sys.stderr)\n\n# Persist canonical price data\nprice_df.to_csv(\"__internal_price_data.csv\", index=False)\nprint(f\"Persisted {len(price_df)} rows to __internal_price_data.csv\", file=sys.stderr)\n\n# Build a date-indexed price series\nprice_df = price_df.set_index(\"date\")\nprice_df.index = pd.to_datetime(price_df.index)\nprice_df.sort_index(inplace=True)\n\n# \u2500\u2500 Step 2: Load yield curve data \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nFRED_URL = \"https://fred.stlouisfed.org/graph/fredgraph.csv?id=T10Y2Y\"\nspread_series = None\n\n# Try local file first\nlocal_file = \"T10Y2Y_spread.csv\"\nif os.path.exists(local_file):\n    print(f\"Loading yield curve data from local file: {local_file}\", file=sys.stderr)\n    try:\n        yc_df = pd.read_csv(local_file, parse_dates=[\"observation_date\"], index_col=\"observation_date\")\n        yc_df.columns = [c.strip() for c in yc_df.columns]\n        # Column might be 'spread_pct' or 'spread'\n        spread_col = [c for c in yc_df.columns if \"spread\" in c.lower()]\n        if spread_col:\n            yc_df[\"spread\"] = pd.to_numeric(yc_df[spread_col[0]], errors=\"coerce\")\n        else:\n            yc_df[\"spread\"] = pd.to_numeric(yc_df.iloc[:, 0], errors=\"coerce\")\n        yc_df.sort_index(inplace=True)\n        spread_series = yc_df[\"spread\"].dropna()\n        print(f\"  Loaded {len(spread_series)} FRED observations from local file.\", file=sys.stderr)\n    except Exception as e:\n        print(f\"  Warning: failed to load local file ({e}), will try FRED URL.\", file=sys.stderr)\n        spread_series = None\n\n# Try FRED URL if local file failed\nif spread_series is None:\n    print(\"Fetching yield curve from FRED...\", file=sys.stderr)\n    try:\n        import urllib.request\n        with urllib.request.urlopen(FRED_URL, timeout=15) as resp:\n            content = resp.read().decode()\n        yc_df = pd.read_csv(\n            io.StringIO(content),\n            parse_dates=[\"observation_date\"],\n            index_col=\"observation_date\"\n        )\n        yc_df.columns = [\"spread\"]\n        yc_df[\"spread\"] = pd.to_numeric(yc_df[\"spread\"], errors=\"coerce\")\n        yc_df.sort_index(inplace=True)\n        spread_series = yc_df[\"spread\"].dropna()\n        print(f\"  {len(spread_series)} FRED observations loaded.\", file=sys.stderr)\n    except Exception as e:\n        print(f\"  Warning: FRED fetch failed ({e}). Will use zeros for spread (no curve filter).\", file=sys.stderr)\n        spread_series = pd.Series(\n            [0.0] * len(price_df),\n            index=price_df.index,\n            name=\"spread\"\n        )\n\n# Filter spread to date range\nspread_series = spread_series.loc[START_DATE:END_DATE]\nprint(f\"  Spread data range: {spread_series.index[0].date()} \u2192 {spread_series.index[-1].date()}\", file=sys.stderr)\n\n# \u2500\u2500 Step 3: Build signals \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\ndf = pd.DataFrame(index=price_df.index)\ndf[\"close\"] = price_df[\"close\"]\n\n# 200-day SMA trend filter\ndf[\"sma200\"] = df[\"close\"].rolling(TREND_WINDOW).mean()\ndf[\"above_trend\"] = (df[\"close\"] > df[\"sma200\"]).astype(int)\n\n# Yield curve regime filter\nspread_ff = spread_series.reindex(df.index, method=\"ffill\")\ndf[\"spread\"] = spread_ff\ndf[\"spread_smooth\"] = spread_ff.rolling(YIELD_SPREAD_SMOOTHING).mean()\ndf[\"curve_ok\"] = (df[\"spread_smooth\"] > YIELD_SPREAD_THRESHOLD).astype(int)\n\n# Combined signal\ndf[\"signal\"] = df[\"above_trend\"] * df[\"curve_ok\"]\n\n# \u2500\u2500 Step 4: Backtest \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\ndf[\"daily_ret\"] = df[\"close\"].pct_change()\ndf[\"position\"] = df[\"signal\"].shift(1).fillna(0)\ndf[\"trades\"] = df[\"position\"].diff().abs().fillna(0)\ndf[\"strat_ret\"] = df[\"position\"] * df[\"daily_ret\"] - df[\"trades\"] * TRANSACTION_COST\ndf[\"bm_ret\"] = df[\"daily_ret\"].fillna(0)\n\n# Drop warm-up period\nwarmup = TREND_WINDOW + YIELD_SPREAD_SMOOTHING + 5\ndf = df.iloc[warmup:].copy()\ndf = df.dropna(subset=[\"close\", \"daily_ret\"]).copy()\n\nprint(f\"Backtest period after warm-up: {df.index[0].date()} \u2192 {df.index[-1].date()}\", file=sys.stderr)\nprint(f\"  Rows: {len(df)}\", file=sys.stderr)\n\n# Equity curves\ndf[\"equity\"] = (1 + df[\"strat_ret\"]).cumprod()\ndf[\"bm_equity\"] = (1 + df[\"bm_ret\"]).cumprod()\n\nbacktest_start = df.index[0].strftime(\"%Y-%m-%d\")\nbacktest_end = df.index[-1].strftime(\"%Y-%m-%d\")\n\nn_years = len(df) / 252.0\n\n# \u2500\u2500 Core strategy metrics \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\ntotal_return = float(df[\"equity\"].iloc[-1] - 1)\ncagr = float(df[\"equity\"].iloc[-1] ** (1 / n_years) - 1) if n_years > 0 else 0.0\nvol = float(df[\"strat_ret\"].std() * np.sqrt(252))\n\nrf = 0.04\nsharpe = float((df[\"strat_ret\"].mean() * 252 - rf) / (vol + 1e-10))\nneg_rets = df[\"strat_ret\"][df[\"strat_ret\"] < 0]\nsortino_neg = neg_rets.std() * np.sqrt(252) if len(neg_rets) > 0 else 1e-10\nsortino = float((df[\"strat_ret\"].mean() * 252 - rf) / (sortino_neg + 1e-10))\n\n# Max drawdown\nrolling_max = df[\"equity\"].cummax()\ndrawdown = df[\"equity\"] / rolling_max - 1\nmax_dd = float(drawdown.min())\n\n# Max drawdown duration\nin_dd = drawdown < 0\ndd_duration = 0\nmax_dd_duration = 0\nfor val in in_dd:\n    if val:\n        dd_duration += 1\n        max_dd_duration = max(max_dd_duration, dd_duration)\n    else:\n        dd_duration = 0\n\ncalmar = float(cagr / abs(max_dd)) if max_dd != 0 else 0.0\n\n# \u2500\u2500 Benchmark metrics \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nbench_total_return = float(df[\"bm_equity\"].iloc[-1] - 1)\nbench_cagr = float(df[\"bm_equity\"].iloc[-1] ** (1 / n_years) - 1) if n_years > 0 else 0.0\n\nbm_rolling_max = df[\"bm_equity\"].cummax()\nbm_drawdown = df[\"bm_equity\"] / bm_rolling_max - 1\nbm_max_dd = float(bm_drawdown.min())\n\n# \u2500\u2500 Trade statistics \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n# Count position changes as trades (entry + exit = 2 trade events per round trip)\nn_trade_events = int(df[\"trades\"].gt(0).sum())\n\n# Extract round-trip trade returns\ntrade_returns = []\nin_trade = False\nentry_eq = 1.0\nfor _, row in df.iterrows():\n    if row[\"position\"] == 1 and not in_trade:\n        in_trade = True\n        entry_eq = row[\"equity\"]\n    elif row[\"position\"] == 0 and in_trade:\n        in_trade = False\n        trade_returns.append(float(row[\"equity\"] / entry_eq - 1))\n\n# If still in trade at end\nif in_trade:\n    trade_returns.append(float(df[\"equity\"].iloc[-1] / entry_eq - 1))\n\ntotal_trades = len(trade_returns) if trade_returns else n_trade_events\n\nwins = [r for r in trade_returns if r > 0]\nlosses = [r for r in trade_returns if r <= 0]\n\nwin_rate = float(len(wins) / len(trade_returns)) if trade_returns else 0.0\navg_win = float(np.mean(wins)) if wins else 0.0\navg_loss = float(np.mean(losses)) if losses else 0.0\nsum_wins = sum(wins) if wins else 0.0\nsum_losses = abs(sum(losses)) if losses else 0.0\nprofit_factor = float(sum_wins / sum_losses) if sum_losses > 0 else (float(\"inf\") if sum_wins > 0 else 0.0)\nif profit_factor == float(\"inf\"):\n    profit_factor = 9999.0\nexpectancy = float(win_rate * avg_win + (1 - win_rate) * avg_loss)\n\n# \u2500\u2500 Alpha / Beta / Information Ratio \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nstrat_r = df[\"strat_ret\"].fillna(0).values\nbm_r = df[\"bm_ret\"].fillna(0).values\n\ncov_matrix = np.cov(strat_r, bm_r)\nbeta = float(cov_matrix[0, 1] / (cov_matrix[1, 1] + 1e-10))\n\n# Jensen's alpha: annualised regression intercept\n# daily alpha = mean(strat_ret) - beta * mean(bm_ret)\ndaily_alpha = np.mean(strat_r) - beta * np.mean(bm_r)\nalpha = float(daily_alpha * 252)\n\n# Excess returns vs benchmark\nexcess_daily = strat_r - bm_r\ninformation_ratio = float(np.mean(excess_daily) / (np.std(excess_daily) + 1e-10) * np.sqrt(252))\nexcess_return = float(cagr - bench_cagr)\n\n# \u2500\u2500 Up/Down Capture \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nup_mask = df[\"bm_ret\"] > 0\ndown_mask = df[\"bm_ret\"] < 0\n\nup_bm_mean = df.loc[up_mask, \"bm_ret\"].mean() if up_mask.sum() > 0 else 0.0\nup_strat_mean = df.loc[up_mask, \"strat_ret\"].mean() if up_mask.sum() > 0 else 0.0\ndown_bm_mean = df.loc[down_mask, \"bm_ret\"].mean() if down_mask.sum() > 0 else 0.0\ndown_strat_mean = df.loc[down_mask, \"strat_ret\"].mean() if down_mask.sum() > 0 else 0.0\n\nup_capture = float(up_strat_mean / up_bm_mean) if up_bm_mean != 0 else 0.0\ndown_capture = float(down_strat_mean / down_bm_mean) if down_bm_mean != 0 else 0.0\n\n# \u2500\u2500 Fetch benchmark data (canonical Alpaca call) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nprint(\"Fetching benchmark buy-and-hold data...\", file=sys.stderr)\nbench_df = _fetch_benchmark_alpaca(\n    ticker=TICKER,\n    start=backtest_start,\n    end=backtest_end,\n    timeframe=\"1Day\",\n    adjustment=\"all\",\n    feed=\"sip\",\n    limit=10000,\n    sort=\"asc\",\n)\nprint(f\"ALPACA_FETCH_OK timeframe=1Day rows={len(bench_df)}\", file=sys.stderr)\n\n# Recompute benchmark metrics from fetched data for consistency\nbench_df = bench_df.set_index(\"date\")\nbench_df.index = pd.to_datetime(bench_df.index)\nbench_df.sort_index(inplace=True)\nbench_df[\"bm_ret\"] = bench_df[\"close\"].pct_change().fillna(0)\nbench_df[\"bm_equity\"] = (1 + bench_df[\"bm_ret\"]).cumprod()\n\nbench_n_years = len(bench_df) / 252.0\nbench_total_return_v2 = float(bench_df[\"bm_equity\"].iloc[-1] - 1)\nbench_cagr_v2 = float(bench_df[\"bm_equity\"].iloc[-1] ** (1 / bench_n_years) - 1) if bench_n_years > 0 else 0.0\nbm_rolling_max_v2 = bench_df[\"bm_equity\"].cummax()\nbm_dd_v2 = bench_df[\"bm_equity\"] / bm_rolling_max_v2 - 1\nbm_max_dd_v2 = float(bm_dd_v2.min())\n\n# Use the benchmark metrics derived from strategy df (same period alignment)\nprint(f\"\\nStrategy CAGR: {cagr:.4f}\", file=sys.stderr)\nprint(f\"Benchmark CAGR: {bench_cagr:.4f}\", file=sys.stderr)\nprint(f\"Max Drawdown: {max_dd:.4f}\", file=sys.stderr)\nprint(f\"Benchmark Max Drawdown: {bm_max_dd:.4f}\", file=sys.stderr)\nprint(f\"Sharpe: {sharpe:.4f}\", file=sys.stderr)\nprint(f\"Total Trades: {total_trades}\", file=sys.stderr)\nprint(f\"Win Rate: {win_rate:.4f}\", file=sys.stderr)\n\n# \u2500\u2500 Build output JSON \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\noutput = {\n    \"backtest_start\": backtest_start,\n    \"backtest_end\": backtest_end,\n    \"cagr\": round(cagr, 6),\n    \"total_return\": round(total_return, 6),\n    \"volatility\": round(vol, 6),\n    \"sharpe_ratio\": round(sharpe, 6),\n    \"sortino_ratio\": round(sortino, 6),\n    \"calmar_ratio\": round(calmar, 6),\n    \"max_drawdown\": round(max_dd, 6),\n    \"max_drawdown_duration\": int(max_dd_duration),\n    \"total_trades\": int(total_trades),\n    \"win_rate\": round(win_rate, 6),\n    \"avg_win\": round(avg_win, 6),\n    \"avg_loss\": round(avg_loss, 6),\n    \"profit_factor\": round(profit_factor, 6),\n    \"expectancy\": round(expectancy, 6),\n    \"benchmark_cagr\": round(bench_cagr, 6),\n    \"benchmark_max_drawdown\": round(bm_max_dd, 6),\n    \"benchmark_total_return\": round(bench_total_return, 6),\n    \"alpha\": round(alpha, 6),\n    \"information_ratio\": round(information_ratio, 6),\n    \"excess_return\": round(excess_return, 6),\n    \"up_capture\": round(up_capture, 6),\n    \"down_capture\": round(down_capture, 6),\n}\n\nprint(json.dumps(output))",
      "stdout": "{\"backtest_start\": \"2021-06-16\", \"backtest_end\": \"2026-02-20\", \"cagr\": 0.036215, \"total_return\": 0.180592, \"volatility\": 0.085352, \"sharpe_ratio\": -0.009076, \"sortino_ratio\": -0.007878, \"calmar_ratio\": 0.230654, \"max_drawdown\": -0.157011, \"max_drawdown_duration\": 931, \"total_trades\": 8, \"win_rate\": 0.375, \"avg_win\": 0.078012, \"avg_loss\": -0.013296, \"profit_factor\": 3.520291, \"expectancy\": 0.020944, \"benchmark_cagr\": 0.125261, \"benchmark_max_drawdown\": -0.24512, \"benchmark_total_return\": 0.734528, \"alpha\": 0.005959, \"information_ratio\": -0.633309, \"excess_return\": -0.089045, \"up_capture\": 0.347858, \"down_capture\": 0.355546}\n",
      "stderr": "Fetching price data for SPY from Alpaca (2005-01-01 \\u2192 2026-02-22)...\nALPACA_FETCH_OK timeframe=1Day rows=1401\nPersisted 1401 rows to __internal_price_data.csv\nLoading yield curve data from local file: T10Y2Y_spread.csv\n  Loaded 12426 FRED observations from local file.\n  Spread data range: 2005-01-03 \\u2192 2026-02-20\nBacktest period after warm-up: 2021-06-16 \\u2192 2026-02-20\n  Rows: 1176\nFetching benchmark buy-and-hold data...\nALPACA_FETCH_OK timeframe=1Day rows=1176\n\nStrategy CAGR: 0.0362\nBenchmark CAGR: 0.1253\nMax Drawdown: -0.1570\nBenchmark Max Drawdown: -0.2451\nSharpe: -0.0091\nTotal Trades: 8\nWin Rate: 0.3750\n",
      "returncode": 0,
      "review_verdict": "success",
      "review_feedback": "",
      "review_message": "",
      "latency_ms": 73812
    }
  ]
}